# Методи декомпозиції матриць: 
  #- SVD-розклад (сингулярний розклад): Матриця розкладається на добуток трьох матриць: 
    # матриці лівих сингулярних векторів, діагональної матриці сингулярних значень та матриці правих сингулярних векторів. 
    # Цей розклад широко використовується в аналізі даних та стисненні інформації.
  #- LU-розклад. Матриця розкладається на добуток нижньої та верхньої трикутних матриць. 
    # Цей розклад часто використовується для розв'язання систем лінійних рівнянь.
  #- Розклад Холецького: Симетрична та позитивно визначена матриця розкладається на добуток нижньої трикутної та її транспонованої. 
  # Цей розклад часто використовується у задачах оптимізації та обчислення кореляційних матриць.

# У лінійній алгебрі існують деякі спеціальні матриці з властивостями, які легко аналізувати та якими легко маніпулювати.
  #- Діагональна матриця (Усі недіагональні елементи діагональної матриці дорівнюють нулю)
  #- Ортогональна матриця QT Q=QQ T = I (оберненою до ортогональної матриці є її транспонувана матриця.)
  #- Симетрична матриця (Матриця є симетричною, якщо її транспонування дорівнює самій собі.)
    # Кожна симетрична матриця S може бути діагоналізована (факторизована) за допомогою матриці, Q сформованої ортонормованими 
    # власними векторами vi матриці S і Λ, яка є діагональною матрицею, що містить всі власні значення. S = QΛQ T 
  #- Ортонормовані власні вектори. Часто ми можемо «вибрати» набір власних векторів, щоб відповідати певним умовам. 
    # Як згадувалося раніше, власні вектори симетричної матриці можна вибрати ортонормованими. 
  #- Додатно визначена матриця (має всі додатні власні значення). Якщо матриця має обернену, її визначник відмінний від нуля. 
    # Визначник додатно визначеної матриці є додатним.
        # Усі власні значення >0.
        # Усі верхні ліві детермінанти >0.
        # Усі опорні елементи верхньої трикутної матриці >0.

# Методи PCA. Сингулярний розклад матриці(SVD) 
    # є ефективним методом зменшення розмірності даних і виявлення головних компонент, які найбільше впливають на варіацію даних.
  #1. Стандартизація даних: Перед застосуванням PCA, дані стандартизуються, щоб кожен атрибут мав середнє значення 0 і стандартне відхилення 1.
  #2. Сингулярний розклад (SVD): Сингулярний розклад дозволяє розкласти матрицю ознак X на добуток трьох матриць X = U⋅Σ⋅V T , де 
    # U і V - ортогональні матриці, а Σ - діагональна матриця сингулярних значень.
  #3. Вибір головних компонент: Сингулярні значення в матриці Σ вказують на важливість кожного вектору в просторі ознак. 
    # Вони впорядковані за спаданням, що дозволяє вибрати перші k найбільших сингулярних значень, що представляють собою головні компоненти.
  #4. Створення нового простору ознак: Обчислюються нові ознаки, які представляють собою проекцію вихідних даних на обрані головні компоненти.
    # Це можна зробити, помноживши матрицю ознак X на матрицю V, скоротивши кількість стовпців до k.
  #5. Відновлення оригінальних даних: Якщо необхідно, новий простір ознак може бути відновлений до вихідного простору ознак, 
    # помноживши його на транспоновану матрицю V.

# Приклад застосування засобами бібліотеки sklearn
from sklearn.decomposition import PCA
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_ratio_)
    # SVD є потужним математичним інструментом, що дозволяє ефективно розкривати структуру даних та виділяти головні компоненти.
    # Це метод факторизації матриці на добуток трьох матриць: A = U Σ V*
	  #-U. Ліві сингулярні вектори A є набором ортонормованих власних векторів AA T
		# Головні компоненти, які отримуються з матриці U в SVD, представляють собою вектори у цьому просторі. 
		# Вони ортогональні один одному і вказують на основні напрямки в розподілі даних.
	  #- Σ. Ненульові сингулярні значення A є квадратними коренями з ненульових власних значень. 
	  	# Вони вказують на важливість кожного з основних напрямків, впорядковані від найважливішого до менш важливого.
	  #- V. Праві сингулярні вектори A є набором ортонормованих власних векторів A T A. 
	  	# Вони вказують, як кожна з осей деформується, щоб утворити оригінальні дані.
  # Сингулярні значення в SVD матриці описують силу лінійних зв’язків між стовпцями та рядками матриці, 
  # що робить SVD корисним інструментом для зменшення розмірності та стиснення даних. 
  # Ортогональні матриці U і V у SVD забезпечують нову основу для матриці, 
  # яку можна використовувати для виконання поворотів і відображень даних.

  # Застосування: 
  # 1. Стиснення даних: зберігаючи в SVD лише найбільші сингулярні значення та відповідні сингулярні вектори, ми можемо зменшити розмір даних, зберігаючи більшість важливої ​​інформації.
  # 2. Обробка зображень: SVD можна використовувати для зменшення шуму на зображеннях, видалення фонового шуму та вилучення важливих функцій із зображень.
  # 3. Рекомендаційні системи: SVD можна використовувати для розкладання матриці елементів користувача на три матриці, які потім можна використовувати для надання рекомендацій користувачам на основі їхньої минулої поведінки.

# Алгоритм:
# 1. Обчислення добутку AA T Якщо матриця A прямокутна, ця операція дозволить отримати квадратну матрицю. Це важливо для подальших операцій.
import numpy as np

# Приклад матриці
# matrix_a = np.array([[1, 2, 3],
#                     [4, 5, 6]])

matrix_a = np.array([[4, 0],
                    [3, -5]])

# Обчислення транспонованої матриці
matrix_a_transposed = np.transpose(matrix_a)  # або matrix_a.T

# Обчислення добутку матриці на її транспоновану
result = np.dot(matrix_a_transposed, matrix_a)  # або matrix_a @ matrix_a_transposed

# Виведення результату
print("Матриця A:")
print(matrix_a)
print("\nТранспонована матриця A:")
print(matrix_a_transposed)
print("\nДобуток матриці A на її транспоновану:")
print(result)
 # Матриця A: 				[[ 4 0] [ 3 -5]]
 # Транспонована матриця A: 	[[ 4 3] [ 0 -5]]
 # Добуток матриці A на її транспоновану:	[[ 25 -15] [-15 25]]


# 2. Обчислення власних значень та векторів.
	#Для цього треба записати характеристичне рівняння та розв'язати його. Детально ця процедура була розглянута у попередній темі.
# Обчислення власних векторів та власних значень
eigenvalues, eigenvectors = np.linalg.eig(result)
# Виведення результату
print("Власні значення:")
print(eigenvalues)
print("\nВласні вектори:")
print(eigenvectors)
# Власні значення: [40. 10.]
# Власні вектори:  [[ 0.70710678 0.70710678] [-0.70710678 0.70710678]]

# 3. Тепер, коли ми отримали власні вектори та власні значення, необхідно скласти матриці V та Σ.
# Отримання індексів, які відсортовують масив за першим стовпцем
sorted_indices = np.argsort(eigenvectors[:, 0])
V = eigenvectors[sorted_indices]

# Виведення результату
print("Вихідний масив:")
print(eigenvectors)
print("\nМатриця V:")
print(V)
# Вихідний масив: 	[[ 0.70710678 0.70710678]  [-0.70710678 0.70710678]]
# Матриця V: 		[[-0.70710678 0.70710678]  [ 0.70710678 0.70710678]]

# діагональні значення в сигмах завжди розташовані в порядку спадання, тому вектори також розміщуються у відповідному порядку.
# Створення діагонального масиву
Sigma = np.diag(sorted(eigenvalues, reverse=True))
Sigma = np.sqrt(Sigma)
print("\nМатриця Σ:")
print(Sigma)
# Матриця Σ: [[6.32455532 0.    ]  [0.     3.16227766]]
Sigma.transpose()
# array([[6.32455532, 0.    ],     [0.    , 3.16227766]])
np.array([[1,2],
          [3,4]]).T
# array([[1, 3],    [2, 4]])

#  головні компоненти відповідають верхньому k-му діагональному елементу, який фіксує найбільшу дисперсію. Чим вище значення, тим важливіший компонент і тим більшу дисперсію він описує.

# 4 Тепер, коли у нас є матриці V і сигма, настав час знайти U.
# 		A = UΣV 		AVΣ = U

AV = np.dot(matrix_a, V)

# Далі нам потрібно перетворити матрицю так, щоб отримати на одиничні вектори у стовпцях матриці.
# Спосіб, яким ми робимо це, беремо значення в стовпцях і ділимо їх на квадратний корінь із суми квадратів значень. 
# Отже, у цьому випадку ми робимо наступне:
def matrix_norm(mtr):
    # Обчислення кореня квадратного з суми елементів для кожного стовпця
    sqrt_sum_columns = np.sqrt(np.sum(mtr**2, axis=0))
    res = mtr / sqrt_sum_columns
    return res
AV = matrix_norm(AV)
print(AV)
	# [[-0.4472136  0.89442719]  [-0.89442719 -0.4472136 ]]
U = np.dot(AV, Sigma.T)
U = matrix_norm(U)
print('Матриця U')
print(U)
	# Матриця U [[-0.4472136  0.89442719]  [-0.89442719 -0.4472136 ]]
    

# Отже, ми обчислили U, сигму та V і розклали матрицю A на три матриці, як наведено нижче.

A = np.dot(np.dot(U, Sigma), V.T)
print('Початкова матриця A')
print(A)

	# Початкова матриця A [[ 4.00000000e+00 2.73432346e-16]  [ 3.00000000e+00 -5.00000000e+00]]
# Порівняємо результат з розкладом за допомогою функції
print(U)
print(Sigma)
print(V)
	# [[-0.4472136  0.89442719] [-0.89442719 -0.4472136 ]]
	# [[6.32455532 0.    ]  [0.     3.16227766]]
	# [[-0.70710678 0.70710678]  [ 0.70710678 0.70710678]]

Uu, Ss, Vh = np.linalg.svd(matrix_a)
print('SVD U ')
print(Uu)
print('SVD Sigma ')
print(Ss)
print('SVD Vh ')
print(Vh)
	# SVD U  [[-0.4472136 -0.89442719]  [-0.89442719 0.4472136 ]]
	# SVD Sigma  [6.32455532 3.16227766]
	# SVD Vh 	[[-0.70710678 0.70710678] [-0.70710678 -0.70710678]]

# Отже, можемо бачити, що результати в цілому співпадають з точністю до знаків деяких векторів

# ------------------------------
# LU-розклад - спосіб розкласти матрицю на множники як добуток нижньої трикутної матриці та верхньої трикутної матриці.
  # Матрицю А можна розкласти на множники нижньої та верхньої трикутної матриці за допомогою елементарних матриць.
    #- Матриця заміни: поміняти місцями рядки i та j одиничної матриці.
    #- Матриця зміни масштабу : якщо потрібно помножити на k рівняння i, помістіть число k у позицію (рядок=i, стовпець=i) одиничної матриці. 
    #- Поворотна матриця: якщо кратне рівняння i додається до рівняння, помістіть число у позицію (рядок=, стовпець=i) одиничної матриці.

import numpy as np

A = np.array([[1, 2, 3],
              [4, -1, 0],
              [-2, 5, 1]])

E1 = np.array([[1,  0, 0],
               [-4, 1, 0],
               [0,  0, 1]])

E2 = np.array([[1, 0, 0],
               [0, 1, 0],
               [2, 0, 1]])

E3 = np.array([[1, 0, 0],
               [0, 1, 0],
               [0, 1, 1]])

E1_inverse = np.linalg.inv(E1)
E2_inverse = np.linalg.inv(E2)
E3_inverse = np.linalg.inv(E3)

U = E3.dot(E2).dot(E1).dot(A)
L = E1_inverse.dot(E2_inverse).dot(E3_inverse)

print("\nStep 1 & 2: Upper traingular matrix of A using elementary matrices:")
print(U)
print("\nStep 1 & 3: Lower traingular matrix of A using inverse elementary matrices:")
print(L)

U_inverse = np.linalg.inv(U)
L_inverse = np.linalg.inv(L)

b1 = np.array([[3],
               [9],
               [-8]]) # column vector

c1 = L_inverse.dot(b1)
x1 = U_inverse.dot(c1)
print("\nStep 4a: Solve c1 given same left hand side matrix A but different right hand side b1:")
print(c1)
print("\nStep 5b: Solution x1 given same left hand side matrix A but different right hand side b1:")
print(x1)

b2 = np.array([[28],
               [22],
               [-11]]) # column vector

c2 = L_inverse.dot(b2)
x2 = U_inverse.dot(c2)
print("\nStep 4a: Solve c2 given same left hand side matrix A but different right hand side b2:")
print(c2)
print("\nStep 5b: Solution x2 given same left hand side matrix A but different right hand side b2:")
print(x2)

# Приклад використання SciPy linalg

import pprint # In order to print matrices prettier
import scipy as sc
import scipy.linalg # Linear Algebra Library contained in Scipy


matrix_A = sc.array([[7,4],[3,5]]) # given matrix A
P, L, U = scipy.linalg.lu(matrix_A) # returns the result of LU decomposition to the variables P, L, and U

print("Original matrix A:")
pprint.pprint(matrix_A)

# implies a pivoting(reordering) rows(or columns) in case it is needed
print("Pivoting matrix P:")
pprint.pprint(P)

# lower-triangular matrix of A
print("L:")
pprint.pprint(L)

# upper-triangular matrix of A
print("U:")
pprint.pprint(U)